"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.LLMClient = void 0;
const openai_1 = require("@langchain/openai");
const messages_1 = require("@langchain/core/messages");
const models_1 = require("./models");
const version_1 = require("../version");
class LLMClient {
    constructor(config) {
        this.config = config;
    }
    createLLM(llmConfig, previousResponseId, extraHeaders) {
        const extraBody = {};
        const thinking = llmConfig.thinking ?? models_1.LLMDefaults.THINKING;
        extraBody.thinking = { type: thinking };
        if (llmConfig.caching && llmConfig.caching !== 'disabled') {
            extraBody.caching = { type: llmConfig.caching };
        }
        const headers = {
            'Authorization': `Bearer ${this.config.apiKey}`,
            'X-Client-Sdk': `coze-coding-dev-sdk-typescript/${version_1.VERSION}`,
            ...extraHeaders,
        };
        if (previousResponseId) {
            headers['X-Previous-Response-Id'] = previousResponseId;
        }
        const llm = new openai_1.ChatOpenAI({
            model: llmConfig.model || models_1.LLMDefaults.MODEL,
            apiKey: this.config.apiKey,
            configuration: {
                baseURL: this.config.modelBaseUrl,
                defaultHeaders: headers,
            },
            streaming: llmConfig.streaming !== undefined ? llmConfig.streaming : models_1.LLMDefaults.STREAMING,
            temperature: llmConfig.temperature !== undefined ? llmConfig.temperature : models_1.LLMDefaults.TEMPERATURE,
            modelKwargs: Object.keys(extraBody).length > 0 ? extraBody : undefined,
        });
        return llm;
    }
    convertMessages(messages) {
        return messages.map(msg => {
            const content = this.convertContent(msg.content);
            switch (msg.role) {
                case 'system':
                    return new messages_1.SystemMessage(content);
                case 'user':
                    return new messages_1.HumanMessage(content);
                case 'assistant':
                    return new messages_1.AIMessage(content);
                default:
                    throw new Error(`Unknown message role: ${msg.role}`);
            }
        });
    }
    convertContent(content) {
        if (typeof content === 'string') {
            return content;
        }
        return content.map(part => {
            if (part.type === 'text') {
                return {
                    type: 'text',
                    text: part.text,
                };
            }
            else if (part.type === 'image_url') {
                const imageUrl = { url: part.image_url.url };
                if (part.image_url.detail) {
                    imageUrl.detail = part.image_url.detail;
                }
                return {
                    type: 'image_url',
                    image_url: imageUrl,
                };
            }
            else if (part.type === 'video_url') {
                const videoUrl = { url: part.video_url.url };
                if (part.video_url.fps !== undefined) {
                    videoUrl.fps = part.video_url.fps;
                }
                return {
                    type: 'video_url',
                    video_url: videoUrl,
                };
            }
            return part;
        });
    }
    async *stream(messages, llmConfig = {}, previousResponseId, extraHeaders) {
        const llm = this.createLLM(llmConfig, previousResponseId, extraHeaders);
        const langchainMessages = this.convertMessages(messages);
        const stream = await llm.stream(langchainMessages);
        for await (const chunk of stream) {
            yield chunk;
        }
    }
    async invoke(messages, llmConfig = {}, previousResponseId, extraHeaders) {
        let fullContent = '';
        const stream = this.stream(messages, llmConfig, previousResponseId, extraHeaders);
        for await (const chunk of stream) {
            if (chunk.content) {
                fullContent += chunk.content.toString();
            }
        }
        return {
            content: fullContent,
        };
    }
}
exports.LLMClient = LLMClient;
//# sourceMappingURL=client.js.map