"use strict";
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.chatCommand = void 0;
const commander_1 = require("commander");
const index_1 = require("../../index");
const chalk_1 = __importDefault(require("chalk"));
const ora_1 = __importDefault(require("ora"));
const utils_1 = require("../utils");
exports.chatCommand = new commander_1.Command('chat')
    .description('Chat with AI using LLM')
    .requiredOption('-p, --prompt <text>', 'User prompt')
    .option('-s, --system <text>', 'System prompt')
    .option('--model <model>', 'Model to use', 'doubao-seed-1-6-251015')
    .option('--temperature <number>', 'Temperature (0-2)', '0.7')
    .option('--stream', 'Enable streaming output', false)
    .option('-H, --header <header>', 'Custom HTTP header (format: "Key: Value" or "Key=Value", can be used multiple times)', (value, previous) => previous.concat([value]), [])
    .option('--verbose', 'Enable verbose logging to print HTTP request details', false)
    .action(async (options) => {
    try {
        const config = new index_1.Config();
        const client = new index_1.LLMClient(config);
        const customHeaders = (0, utils_1.parseHeaders)(options.header);
        const messages = [];
        if (options.system) {
            messages.push({ role: 'system', content: options.system });
        }
        messages.push({ role: 'user', content: options.prompt });
        const llmConfig = {
            model: options.model,
            temperature: parseFloat(options.temperature),
        };
        if (options.stream) {
            console.log(chalk_1.default.cyan('\nAI Response:'));
            const stream = client.stream(messages, llmConfig, undefined, customHeaders);
            for await (const chunk of stream) {
                if (chunk.content) {
                    process.stdout.write(chunk.content.toString());
                }
            }
            console.log('\n');
        }
        else {
            const spinner = (0, ora_1.default)('Generating response...').start();
            const response = await client.invoke(messages, llmConfig, undefined, customHeaders);
            spinner.succeed(chalk_1.default.green('Response generated!'));
            console.log(chalk_1.default.cyan('\nAI Response:'));
            console.log(response.content);
        }
    }
    catch (error) {
        console.error(chalk_1.default.red('Chat failed'));
        console.error(chalk_1.default.red(error.message));
        process.exit(1);
    }
});
//# sourceMappingURL=chat.js.map